# üåê –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥–∞

–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ HTML —Å—Ç—Ä–∞–Ω–∏—Ü —Å –≤–µ–±-—Å–∞–π—Ç–æ–≤. –°–æ–∑–¥–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–µ–≥–æ —Å—Ç–∞—Ä–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞ `downloader.ipynb`, –Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω.

## ‚ú® –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- **–ó–∞–≥—Ä—É–∑–∫–∞ HTML —Å—Ç—Ä–∞–Ω–∏—Ü** —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏
- **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–µ—Å—Å–∏—è–º–∏** —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
- **–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫** –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏
- **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∑–æ–∫** —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –æ—Ç—á—ë—Ç–∞–º–∏
- **–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤** –≤ CSV –∏ JSON —Ñ–æ—Ä–º–∞—Ç–∞—Ö
- **–ì–∏–±–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞** User-Agent –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
tools/web_scraper/
‚îú‚îÄ‚îÄ __init__.py              # –û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å
‚îú‚îÄ‚îÄ html_downloader.py       # –ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü
‚îú‚îÄ‚îÄ scraping_manager.py      # –ú–µ–Ω–µ–¥–∂–µ—Ä —Å–µ—Å—Å–∏–π —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
‚îú‚îÄ‚îÄ main.py                  # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç
‚îî‚îÄ‚îÄ README.md                # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
```

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã:

```python
from tools.web_scraper import HTMLDownloader

# –°–æ–∑–¥–∞—ë–º –∑–∞–≥—Ä—É–∑—á–∏–∫
downloader = HTMLDownloader(
    base_url="https://example.com",
    save_path="./downloads"
)

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
if downloader.download_page(filename="example.html"):
    print("–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

# –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
stats = downloader.get_stats()
print(f"–£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫: {stats['successful']}")

downloader.close()
```

### –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü:

```python
# –ó–∞–≥—Ä—É–∂–∞–µ–º 10 —Å—Ç—Ä–∞–Ω–∏—Ü —Å –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏
downloaded_files = downloader.download_multiple_pages(
    num_pages=10,
    filename_pattern="page_{}.html",
    delay=True
)

print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(downloaded_files)}")
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞:

```python
from tools.web_scraper import ScrapingManager

# –°–æ–∑–¥–∞—ë–º –º–µ–Ω–µ–¥–∂–µ—Ä
manager = ScrapingManager(
    base_url="https://example.com",
    save_path="./scraping_results"
)

# –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Å—Å–∏—é
session_result = manager.start_scraping_session(
    session_name="my_session",
    num_pages=5,
    delay_range=(2, 5)
)

# –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
manager.export_metadata_to_csv("summary.csv")
```

## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏

### HTMLDownloader –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:

- **`base_url`** - –ë–∞–∑–æ–≤—ã–π URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
- **`save_path`** - –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
- **`delay_range`** - –î–∏–∞–ø–∞–∑–æ–Ω –∑–∞–¥–µ—Ä–∂–µ–∫ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
- **`max_retries`** - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
- **`user_agent`** - –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π User-Agent

### ScrapingManager –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:

- **`base_url`** - –ë–∞–∑–æ–≤—ã–π URL –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
- **`save_path`** - –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- **`metadata_file`** - –ò–º—è —Ñ–∞–π–ª–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –æ—Ç—á—ë—Ç—ã

### –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:

```python
# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
stats = downloader.get_stats()
print(f"–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {stats['success_rate_percent']}%")

# –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–µ–Ω–µ–¥–∂–µ—Ä–∞
total_stats = manager.get_total_stats()
print(f"–í—Å–µ–≥–æ —Å–µ—Å—Å–∏–π: {total_stats['total_sessions']}")
```

### –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö:

```python
# –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV
manager.export_metadata_to_csv("scraping_summary.csv")

# –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ JSON
# –§–∞–π–ª: scraping_results/scraping_metadata.json
```

## üõ°Ô∏è –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —ç—Ç–∏–∫–∞

- **–£–≤–∞–∂–∞–π—Ç–µ robots.txt** - –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –ø—Ä–∞–≤–∏–ª–∞ —Å–∞–π—Ç–∞
- **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∑–∞–¥–µ—Ä–∂–∫–∏** - –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–π—Ç–µ —Å–µ—Ä–≤–µ—Ä—ã
- **–°–æ–±–ª—é–¥–∞–π—Ç–µ –ª–∏–º–∏—Ç—ã** - –Ω–µ –¥–µ–ª–∞–π—Ç–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤
- **–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ Terms of Service** - —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–∫—Ä–∞–ø–∏–Ω–≥ —Ä–∞–∑—Ä–µ—à—ë–Ω

## üîß –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤ –ø–æ –≤–æ–∂–¥–µ–Ω–∏—é (–∫–∞–∫ –≤ –≤–∞—à–µ–º —Å—Ç–∞—Ä–æ–º —Å–∫—Ä–∏–ø—Ç–µ):

```python
downloader = HTMLDownloader(
    base_url="https://practicatest.com/tests/permiso-B/online",
    save_path="./driving_tests",
    delay_range=(3, 7)  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –¥–ª—è —É–≤–∞–∂–µ–Ω–∏—è —Å–µ—Ä–≤–µ—Ä–∞
)

# –ó–∞–≥—Ä—É–∂–∞–µ–º 50 —Å—Ç—Ä–∞–Ω–∏—Ü
downloaded_files = downloader.download_multiple_pages(
    num_pages=50,
    filename_pattern="test_page_{}.html"
)
```

### 2. –°–∫—Ä–∞–ø–∏–Ω–≥ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:

```python
manager = ScrapingManager(
    base_url="https://api.example.com/search",
    save_path="./search_results"
)

# –°–ø–∏—Å–æ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
search_params = [
    {"query": "spanish", "level": "beginner"},
    {"query": "spanish", "level": "intermediate"},
    {"query": "spanish", "level": "advanced"}
]

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
session_result = manager.scrape_with_parameters(
    session_name="spanish_search",
    parameters_list=search_params
)
```

## üö® –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç:
- **–°–µ—Ç–µ–≤—ã–µ –æ—à–∏–±–∫–∏** - –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏
- **HTTP –æ—à–∏–±–∫–∏** - –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- **–¢–∞–π–º–∞—É—Ç—ã** - –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –ª–∏–º–∏—Ç—ã
- **–û—à–∏–±–∫–∏ –∑–∞–ø–∏—Å–∏** - –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```
scraping_results/
‚îú‚îÄ‚îÄ session_name_1/
‚îÇ   ‚îú‚îÄ‚îÄ page_1.html
‚îÇ   ‚îú‚îÄ‚îÄ page_2.html
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ session_name_2/
‚îÇ   ‚îú‚îÄ‚îÄ params_1.html
‚îÇ   ‚îî‚îÄ‚îÄ params_2.html
‚îú‚îÄ‚îÄ scraping_metadata.json
‚îî‚îÄ‚îÄ scraping_summary.csv
```

## üéØ –ó–∞–º–µ–Ω–∞ —Å—Ç–∞—Ä–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞

–í–º–µ—Å—Ç–æ —Å—Ç–∞—Ä–æ–≥–æ `downloader.ipynb` —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ:

```python
# –°—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–± (–∏–∑ downloader.ipynb):
# –ü—Ä–æ—Å—Ç–æ–π —Ü–∏–∫–ª —Å requests.get()

# –ù–æ–≤—ã–π —Å–ø–æ—Å–æ–±:
from tools.web_scraper import HTMLDownloader

downloader = HTMLDownloader(
    base_url="https://practicatest.com/tests/permiso-B/online",
    save_path="./data/driving_tests"
)

downloaded_files = downloader.download_multiple_pages(
    num_pages=50,
    delay_range=(2, 5)
)
```

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

–ó–∞–ø—É—Å—Ç–∏—Ç–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç:

```bash
cd tools/web_scraper
python main.py
```

–≠—Ç–æ —Å–æ–∑–¥–∞—Å—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø–æ–∫–∞–∂–µ—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞.

## üìù –õ–∏—Ü–µ–Ω–∑–∏—è

MIT License - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–≤–æ–±–æ–¥–Ω–æ –¥–ª—è –ª—é–±—ã—Ö —Ü–µ–ª–µ–π.
