#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –±–∏–ª–µ—Ç–æ–≤ –ø–æ –≤–æ–∂–¥–µ–Ω–∏—é

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç HTML —Ñ–∞–π–ª—ã —Å –±–∏–ª–µ—Ç–∞–º–∏ –∏ —Å–æ–∑–¥–∞—ë—Ç Excel –æ—Ç—á—ë—Ç—ã
"""

import os
import sys
import logging
from pathlib import Path
from datetime import datetime
import time

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º –ø—Ä–æ–µ–∫—Ç–∞
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from src.spanish_analyser.word_analyzer import WordAnalyzer
from src.spanish_analyser.anki_integration import AnkiIntegration
from src.spanish_analyser.config import config


class DrivingTestsAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –±–∏–ª–µ—Ç–æ–≤ –ø–æ –≤–æ–∂–¥–µ–Ω–∏—é"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        self.word_analyzer = WordAnalyzer()
        self.anki_integration = AnkiIntegration()
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.downloads_path = Path(config.get_downloads_folder())
        self.results_path = Path(config.get_results_folder())
        self.max_results_files = config.get_max_results_files()
        self.results_filename_prefix = config.get_results_filename_prefix()
        
        # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.results_path.mkdir(parents=True, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        from src.spanish_analyser.text_processor import SpanishTextProcessor
        self.text_processor = SpanishTextProcessor()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞
        self.analysis_stats = {
            'files_processed': 0,
            'words_found': 0,
            'start_time': datetime.now()
        }
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging.basicConfig(
            level=getattr(logging, config.get_logging_level()),
            format=config.get_logging_format()
        )
        self.logger = logging.getLogger(__name__)
        
        self.logger.info("–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        self.logger.info(f"–ü–∞–ø–∫–∞ –∑–∞–≥—Ä—É–∑–æ–∫: {self.downloads_path}")
        self.logger.info(f"–ü–∞–ø–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {self.results_path}")
    
    def connect_to_anki(self) -> bool:
        """
        –ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ Anki –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
        
        Returns:
            True –µ—Å–ª–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
        """
        try:
            self.logger.info("–ü–æ–¥–∫–ª—é—á–∞—é—Å—å –∫ Anki...")
            if self.anki_integration.connect():
                self.logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Anki —É—Å–ø–µ—à–Ω–æ")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∏—Å–ø–∞–Ω—Å–∫–∏—Ö –∫–æ–ª–æ–¥
                self.logger.info("–ó–∞–≥—Ä—É–∂–∞—é –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∫–æ–ª–æ–¥ Spanish...")
                if self.word_analyzer.load_known_words_from_anki(
                    self.anki_integration, 
                    deck_pattern="Spanish*",
                    field_names=['FrontText', 'BackText']
                ):
                    self.logger.info("‚úÖ –ò–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ Anki")
                    return True
                else:
                    self.logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ Anki")
                    return False
            else:
                self.logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Anki")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –∫ Anki: {e}")
            return False
    
    def find_html_files(self, pattern: str = "*.html") -> list:
        """
        –ù–∞—Ö–æ–¥–∏—Ç HTML —Ñ–∞–π–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            pattern: –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ HTML —Ñ–∞–π–ª–∞–º
        """
        html_files = list(self.downloads_path.glob(pattern))
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(html_files)} HTML —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return html_files
    
    def extract_text_from_html(self, html_file: Path) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ HTML —Ñ–∞–π–ª–∞
        
        Args:
            html_file: –ü—É—Ç—å –∫ HTML —Ñ–∞–π–ª—É
            
        Returns:
            –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        try:
            with open(html_file, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
            cleaned_text = self._extract_text_improved(html_content)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –∏—Å–ø–∞–Ω—Å–∫–∏–µ —Å–ª–æ–≤–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            spanish_words = self.text_processor.extract_spanish_words(cleaned_text)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            final_text = ' '.join(spanish_words)
            
            self.logger.debug(f"–ò–∑–≤–ª–µ—á—ë–Ω —Ç–µ–∫—Å—Ç –∏–∑ {html_file.name}: {len(final_text)} —Å–∏–º–≤–æ–ª–æ–≤, {len(spanish_words)} —Å–ª–æ–≤")
            return final_text
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ {html_file.name}: {e}")
            return ""
    
    def _extract_text_improved(self, html_content: str) -> str:
        """
        –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML —Å –ø–æ–∏—Å–∫–æ–º –±–ª–æ–∫–æ–≤ col-md-8
        
        Args:
            html_content: HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            
        Returns:
            –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        try:
            from bs4 import BeautifulSoup
            
            # –°–æ–∑–¥–∞—ë–º –æ–±—ä–µ–∫—Ç BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –±–ª–æ–∫–∏ —Å –∫–ª–∞—Å—Å–æ–º "col-md-8" (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –∫–æ–¥–µ)
            blocks = soup.find_all('div', class_='col-md-8')
            
            if blocks:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö
                text = "\n".join([block.get_text(separator=" ", strip=True) for block in blocks])
                self.logger.debug(f"–ù–∞–π–¥–µ–Ω–æ {len(blocks)} –±–ª–æ–∫–æ–≤ col-md-8")
            else:
                # Fallback: –∏—â–µ–º –¥—Ä—É–≥–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º
                text_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'div'])
                text = "\n".join([elem.get_text(strip=True) for elem in text_elements if elem.get_text(strip=True)])
                self.logger.debug(f"Fallback: –Ω–∞–π–¥–µ–Ω–æ {len(text_elements)} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            
            return text.strip()
            
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–Ω–æ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            # Fallback –∫ –±–∞–∑–æ–≤–æ–º—É –º–µ—Ç–æ–¥—É
            return self.text_processor.clean_text(html_content, remove_prefixes=False)
    
    def analyze_html_files(self, html_files: list = None) -> dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç HTML —Ñ–∞–π–ª—ã –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ª–æ–≤–∞
        
        Args:
            html_files: –°–ø–∏—Å–æ–∫ HTML —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –∞–Ω–∞–ª–∏–∑–∞
        """
        if html_files is None:
            html_files = self.find_html_files()
        
        self.logger.info(f"–ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ {len(html_files)} HTML —Ñ–∞–π–ª–æ–≤")
        
        total_words = 0
        
        for html_file in html_files:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                text = self.extract_text_from_html(html_file)
                if text:
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –≤ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
                    self.word_analyzer.add_words_from_text(text)
                    
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–ª–æ–≤–∞ –≤ —ç—Ç–æ–º —Ñ–∞–π–ª–µ
                    words_in_file = len(text.split())
                    total_words += words_in_file
                    
                    self.analysis_stats['files_processed'] += 1
                    self.logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω {html_file.name}: –Ω–∞–π–¥–µ–Ω–æ {words_in_file} —Å–ª–æ–≤")
                else:
                    self.logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω {html_file.name}: –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
                    
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {html_file.name}: {e}")
        
        self.analysis_stats['words_found'] = total_words
        
        self.logger.info(f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {self.analysis_stats['files_processed']}")
        self.logger.info(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤: {total_words}")
        
        return {
            'files_processed': self.analysis_stats['files_processed'],
            'words_found': total_words,
            'unique_words': len(self.word_analyzer.word_frequencies)
        }
    
    def generate_filename_with_timestamp(self, prefix: str = "driving_tests_analysis") -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
        
        Args:
            prefix: –ü—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            
        Returns:
            –ò–º—è —Ñ–∞–π–ª–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"{prefix}_{timestamp}.xlsx"
    
    def cleanup_old_files(self):
        """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –æ—Å—Ç–∞–≤–ª—è—è –Ω–µ –±–æ–ª–µ–µ max_files"""
        try:
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ Excel —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            excel_files = list(self.results_path.glob("*.xlsx"))
            
            if len(excel_files) > self.max_results_files:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è (—Å—Ç–∞—Ä—ã–µ –ø–µ—Ä–≤—ã–º–∏)
                excel_files.sort(key=lambda x: x.stat().st_mtime)
                
                # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã
                files_to_delete = excel_files[:-self.max_results_files]
                
                for old_file in files_to_delete:
                    old_file.unlink()
                    self.logger.info(f"–£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª: {old_file.name}")
                
                self.logger.info(f"–£–¥–∞–ª–µ–Ω–æ {len(files_to_delete)} —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")
    
    def export_results(self, include_categories: bool = True) -> str:
        """
        –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ Excel
        
        Args:
            include_categories: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —á–∞—Å—Ç–æ—Ç–µ
            
        Returns:
            –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
            filename = self.generate_filename_with_timestamp("driving_tests_analysis")
            file_path = self.results_path / filename
            
            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self.word_analyzer.export_to_excel(str(file_path), include_categories)
            
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã
            self.cleanup_old_files()
            
            self.logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤: {filename}")
            return str(file_path)
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
            return ""
    
    def get_analysis_summary(self) -> dict:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –∞–Ω–∞–ª–∏–∑—É"""
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ç WordAnalyzer
        word_stats = self.word_analyzer.get_summary_stats()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—à—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        analysis_summary = {
            'files_processed': self.analysis_stats['files_processed'],
            'words_found': self.analysis_stats['words_found'],
            'unique_words': word_stats['–≤—Å–µ–≥–æ_—É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö_—Å–ª–æ–≤'],
            'known_words': word_stats['–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö_—Å–ª–æ–≤'],
            'new_words': word_stats['–Ω–æ–≤—ã—Ö_—Å–ª–æ–≤'],
            'analysis_time': str(datetime.now() - self.analysis_stats['start_time'])
        }
        
        return analysis_summary
    
    def reset_analysis(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞"""
        self.word_analyzer.reset()
        self.analysis_stats = {
            'files_processed': 0,
            'words_found': 0,
            'start_time': datetime.now()
        }
        self.logger.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Å–±—Ä–æ—à–µ–Ω—ã")
    
    def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if self.anki_integration:
            self.anki_integration.disconnect()
            self.logger.info("–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Anki –∑–∞–∫—Ä—ã—Ç–æ")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    print("üìä –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –±–∏–ª–µ—Ç–æ–≤ –ø–æ –≤–æ–∂–¥–µ–Ω–∏—é\n")
    
    # –°–æ–∑–¥–∞—ë–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer = DrivingTestsAnalyzer()
    
    try:
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Anki
        print("üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Anki...")
        if not analyzer.connect_to_anki():
            print("‚ö†Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ Anki...")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º HTML —Ñ–∞–π–ª—ã
        print("\nüìÑ –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ HTML —Ñ–∞–π–ª–æ–≤...")
        analysis_result = analyzer.analyze_html_files()
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:")
        print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {analysis_result['files_processed']}")
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤: {analysis_result['words_found']}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {analysis_result['unique_words']}")
        
        # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"\nüìÅ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")
        export_file = analyzer.export_results()
        
        if export_file:
            print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤: {export_file}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–≤–æ–¥–∫—É
        summary = analyzer.get_analysis_summary()
        print(f"\nüìã –°–≤–æ–¥–∫–∞ –∞–Ω–∞–ª–∏–∑–∞:")
        print(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {summary['analysis_time']}")
        print(f"   –ò–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤: {summary['known_words']}")
        print(f"   –ù–æ–≤—ã—Ö —Å–ª–æ–≤: {summary['new_words']}")
        
    except Exception as e:
        print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        return 1
    finally:
        analyzer.close()
    
    return 0


if __name__ == "__main__":
    exit(main())
